{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Dataset\n",
    "\n",
    "We manually annotated the sample with the annotator tool Prodigy, and saved the annotations in a file called 'prodigy_manual_annotations' which is stored inside prodigy.db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sb02767\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sb02767\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.5 MB 5.5 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/11.5 MB 7.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/11.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.0/11.5 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.5/11.5 MB 17.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.6/11.5 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.3/11.5 MB 27.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 36.4 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "   ---------------------------------------- 0.0/346.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 346.8/346.8 kB ? eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "# Load libraries and set working directory to where files are stored\n",
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_transformers\n",
    "from spacy.tokens import DocBin\n",
    "import srsly\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")  # Load the spaCy model that corresponds to your use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_bin = DocBin()  # This will store the Doc objects\n",
    "data = srsly.read_jsonl('gold_standard.jsonl')  # Load your Prodigy annotations\n",
    "\n",
    "for example in data:\n",
    "    text = example['text']\n",
    "    annotations = example['spans']\n",
    "    doc = nlp.make_doc(text)  # Create a Doc object from text\n",
    "    ents = []\n",
    "    for span in annotations:\n",
    "        start, end, label = span['start'], span['end'], span['label']\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents  # Assign the entity annotations\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "# doc_bin.to_disk(\"./manual.spacy\")  # Save the DocBin to disk\n",
    "\n",
    "# Load the DocBin object\n",
    "doc_bin = DocBin().from_disk(\"manual.spacy\")\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These instead are the entities that were annotated by the model\n",
    "ner_entities = pd.read_csv(\"ner_entities_only_relevant_domains.csv\")\n",
    "\n",
    "# Filter the 'ner_entities' DataFrame to keep only the rows where 'doc' matches the gold standard sample\n",
    "ner_entities_sample = ner_entities[ner_entities['doc'].isin(sample['text'])]\n",
    "\n",
    "from spacy.training import Example\n",
    "docs = [nlp(text) for text in validation_ner]\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk(\"bert.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ner = DocBin().from_disk(\"bert.spacy\")\n",
    "docs_ner = list(docs_ner.get_docs(nlp.vocab))\n",
    "\n",
    "def process_doc(doc):\n",
    "    entities = {\"GPE\": [], \"LOC\": [], \"FAC\": [], \"ORG\": []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:  # Check if the entity type is one we want to collect\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    return {\"doc\": doc.text, \"entities\": entities}\n",
    "\n",
    "processed_docs_gold = [process_doc(doc) for doc in docs]\n",
    "processed_docs_ner = [process_doc(doc) for doc in docs_ner]\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "df_gold = pd.DataFrame(processed_docs_gold)\n",
    "df_ner = pd.DataFrame(processed_docs_ner)\n",
    "\n",
    "# Merge the DataFrames on the 'doc' column\n",
    "merged_df = pd.merge(df_gold, df_ner, on='doc', suffixes=('_gold', '_ner'))\n",
    "# merged_df.to_csv(\"gold_standard_entities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toponym Recognition Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    df = df.copy()  # Avoid working with a slice of the original DataFrame\n",
    "\n",
    "    def compare_entities(row):\n",
    "        # Convert both arrays to sets for easier comparison\n",
    "        gold_set = set(row['gold_entities_array'])\n",
    "        machine_set = set(row['machine_entities_array'])\n",
    "        \n",
    "        # Calculate True Positives (TP), False Positives (FP), False Negatives (FN)\n",
    "        tp = len(gold_set & machine_set)\n",
    "        fp = len(machine_set - gold_set)\n",
    "        fn = len(gold_set - machine_set)\n",
    "        \n",
    "        return pd.Series([tp, fp, fn])\n",
    "\n",
    "    # Apply the function across the dataframe and create new columns\n",
    "    df[['True Positives', 'False Positives', 'False Negatives']] = df.apply(compare_entities, axis=1)\n",
    "\n",
    "    # Optionally, calculate precision, recall, and F1-score\n",
    "    df['Precision'] = df['True Positives'] / (df['True Positives'] + df['False Positives'])\n",
    "    df['Recall'] = df['True Positives'] / (df['True Positives'] + df['False Negatives'])\n",
    "    \n",
    "    # Replace NaN (which results from division by zero) with 1\n",
    "    df['Precision'].replace(np.nan, 1, inplace=True)\n",
    "    df['Recall'].replace(np.nan, 1, inplace=True)\n",
    "\n",
    "    df['F1-Score'] = 2 * (df['Precision'] * df['Recall']) / (df['Precision'] + df['Recall'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"hand_annotation_vs_spacy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert JSON-like strings to dictionaries\n",
    "def safe_literal_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return {}  # or np.nan, or handle as needed\n",
    "\n",
    "# Convert JSON strings in 'entities_gold' to dictionaries\n",
    "merged_df['entities_gold'] = merged_df['entities_gold'].apply(safe_literal_eval)\n",
    "\n",
    "# Convert JSON strings in 'entities_gold' to dictionaries\n",
    "merged_df['entities_ner'] = merged_df['entities_ner'].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrame to store all results\n",
    "articles_entities = pd.DataFrame()\n",
    "\n",
    "def preprocess_entities(df, gold_col='entities_gold', machine_col='entities_ner'):\n",
    "\n",
    "    # Extract and create GPE, LOC, FAC columns from 'entities_gold'\n",
    "    for etype in ['GPE', 'LOC', 'FAC']:\n",
    "        df[etype] = df[gold_col].apply(lambda x: x.get(etype, []))\n",
    "\n",
    "    # Extract and create machine_GPE, machine_LOC, machine_FAC columns from 'entities_ner'\n",
    "    for etype in ['GPE', 'LOC', 'FAC']:\n",
    "        df[f'machine_{etype}'] = df[machine_col].apply(lambda x: x.get(etype, []))\n",
    "    \n",
    "    # Convert empty lists to NaN for both sets of columns\n",
    "    df[['GPE', 'LOC', 'FAC']] = df[['GPE', 'LOC', 'FAC']].map(lambda x: np.nan if not x else x)\n",
    "    df[[f\"machine_{etype}\" for etype in ['GPE', 'LOC', 'FAC']]] = df[[f\"machine_{etype}\" for etype in ['GPE', 'LOC', 'FAC']]].map(lambda x: np.nan if not x else x)\n",
    "\n",
    "    # Create arrays (lists) for the gold and machine entities\n",
    "    df['gold_entities_array'] = df.apply(lambda row: [row['GPE'], row['LOC'], row['FAC']], axis=1)\n",
    "    df['machine_entities_array'] = df.apply(lambda row: [row['machine_GPE'], row['machine_LOC'], row['machine_FAC']], axis=1)\n",
    "    \n",
    "    # Flatten the arrays and remove NaNs\n",
    "    df['gold_entities_array'] = df['gold_entities_array'].apply(lambda x: [item for sublist in x if isinstance(sublist, list) for item in sublist])\n",
    "    df['machine_entities_array'] = df['machine_entities_array'].apply(lambda x: [item for sublist in x if isinstance(sublist, list) for item in sublist])\n",
    "    \n",
    "    # Select only the relevant columns\n",
    "    df = df[['doc', 'gold_entities_array', 'machine_entities_array']]\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    df = df.copy()  # Avoid working with a slice of the original DataFrame\n",
    "\n",
    "    def compare_entities(row):\n",
    "        # Convert both arrays to sets for easier comparison\n",
    "        gold_set = set(row['gold_entities_array'])\n",
    "        machine_set = set(row['machine_entities_array'])\n",
    "        \n",
    "        # Calculate True Positives (TP), False Positives (FP), False Negatives (FN)\n",
    "        tp = len(gold_set & machine_set)\n",
    "        fp = len(machine_set - gold_set)\n",
    "        fn = len(gold_set - machine_set)\n",
    "        \n",
    "        return pd.Series([tp, fp, fn])\n",
    "\n",
    "    # Apply the function across the dataframe and create new columns\n",
    "    df[['True Positives', 'False Positives', 'False Negatives']] = df.apply(compare_entities, axis=1)\n",
    "\n",
    "    # Optionally, calculate precision, recall, and F1-score\n",
    "    df['Precision'] = df['True Positives'] / (df['True Positives'] + df['False Positives'])\n",
    "    df['Recall'] = df['True Positives'] / (df['True Positives'] + df['False Negatives'])\n",
    "    \n",
    "    # Replace NaN (which results from division by zero) with 1\n",
    "    df['Precision'].replace(np.nan, 1, inplace=True)\n",
    "    df['Recall'].replace(np.nan, 1, inplace=True)\n",
    "\n",
    "    df['F1-Score'] = 2 * (df['Precision'] * df['Recall']) / (df['Precision'] + df['Recall'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Precision: 0.9659735349716446\n",
      "Overall Recall: 0.9092526690391459\n",
      "Overall F1-Score: 0.9367552703941339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sb02767\\AppData\\Local\\Temp\\ipykernel_36816\\586724846.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Precision'].replace(np.nan, 1, inplace=True)\n",
      "C:\\Users\\sb02767\\AppData\\Local\\Temp\\ipykernel_36816\\586724846.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Recall'].replace(np.nan, 1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "processed_df = preprocess_entities(merged_df)\n",
    "metrics = calculate_metrics(processed_df)\n",
    "\n",
    "# Sum TP, FP, FN across all documents\n",
    "total_tp = metrics['True Positives'].sum()\n",
    "total_fp = metrics['False Positives'].sum()\n",
    "total_fn = metrics['False Negatives'].sum()\n",
    "\n",
    "# Micro-averaged (overall) precision, recall, and F1-score\n",
    "overall_precision = total_tp / (total_tp + total_fp)\n",
    "overall_recall = total_tp / (total_tp + total_fn)\n",
    "overall_f1_score = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall)\n",
    "\n",
    "print(f\"Overall Precision: {overall_precision}\")\n",
    "print(f\"Overall Recall: {overall_recall}\")\n",
    "print(f\"Overall F1-Score: {overall_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>gold_entities_array</th>\n",
       "      <th>machine_entities_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNOWLE and Dorridge CC suffered a heavy 160-ru...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>now Wheldon Infant School and Nursery on Franc...</td>\n",
       "      <td>[Castleford, Francis Street]</td>\n",
       "      <td>[Castleford, Francis Street]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Council leaders in West Norfolk are set to exa...</td>\n",
       "      <td>[West Norfolk, West Norfolk]</td>\n",
       "      <td>[West Norfolk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our 10 photographs from the ChronicleLive arch...</td>\n",
       "      <td>[Newcastle, Newcastle, Newcastle, Newcastle, N...</td>\n",
       "      <td>[Newcastle, Newcastle, Newcastle, Newcastle, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Antrim and Newtownabbey Borough Council has al...</td>\n",
       "      <td>[Antrim, Newtownabbey, Newtownabbey, Antrim, N...</td>\n",
       "      <td>[Antrim, Newtownabbey, Newtownabbey, Antrim, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Young Charlton Athletic forward Ryan Viggars h...</td>\n",
       "      <td>[Wales, Bloomfields]</td>\n",
       "      <td>[Wales, Bloomfields]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The following planning applications have been ...</td>\n",
       "      <td>[Newark, Newark, Balderton, Bilsthorpe, Morton...</td>\n",
       "      <td>[Newark, Newark, Balderton, Bilsthorpe, Morton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Easier access to your trusted, local news. Sub...</td>\n",
       "      <td>[Rothes, Wick, Mosset Park]</td>\n",
       "      <td>[Rothes, Wick, Mosset Park]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>[Alcester, Alcester, the Town Hall, Hertford, ...</td>\n",
       "      <td>[Alcester, Alcester, the Town Hall]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  doc  \\\n",
       "0   Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "1   KNOWLE and Dorridge CC suffered a heavy 160-ru...   \n",
       "2   now Wheldon Infant School and Nursery on Franc...   \n",
       "3   Council leaders in West Norfolk are set to exa...   \n",
       "4   Our 10 photographs from the ChronicleLive arch...   \n",
       "..                                                ...   \n",
       "95  Antrim and Newtownabbey Borough Council has al...   \n",
       "96  Young Charlton Athletic forward Ryan Viggars h...   \n",
       "97  The following planning applications have been ...   \n",
       "98  Easier access to your trusted, local news. Sub...   \n",
       "99  A group of around 20 women and men protested a...   \n",
       "\n",
       "                                  gold_entities_array  \\\n",
       "0                                                  []   \n",
       "1                                                  []   \n",
       "2                        [Castleford, Francis Street]   \n",
       "3                        [West Norfolk, West Norfolk]   \n",
       "4   [Newcastle, Newcastle, Newcastle, Newcastle, N...   \n",
       "..                                                ...   \n",
       "95  [Antrim, Newtownabbey, Newtownabbey, Antrim, N...   \n",
       "96                               [Wales, Bloomfields]   \n",
       "97  [Newark, Newark, Balderton, Bilsthorpe, Morton...   \n",
       "98                        [Rothes, Wick, Mosset Park]   \n",
       "99  [Alcester, Alcester, the Town Hall, Hertford, ...   \n",
       "\n",
       "                               machine_entities_array  \n",
       "0                                                  []  \n",
       "1                                                  []  \n",
       "2                        [Castleford, Francis Street]  \n",
       "3                                      [West Norfolk]  \n",
       "4   [Newcastle, Newcastle, Newcastle, Newcastle, N...  \n",
       "..                                                ...  \n",
       "95  [Antrim, Newtownabbey, Newtownabbey, Antrim, N...  \n",
       "96                               [Wales, Bloomfields]  \n",
       "97  [Newark, Newark, Balderton, Bilsthorpe, Morton...  \n",
       "98                        [Rothes, Wick, Mosset Park]  \n",
       "99                [Alcester, Alcester, the Town Hall]  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toponym Disambiguation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast  # For safely evaluating string representations of dictionaries\n",
    "\n",
    "# Load CSV file\n",
    "merged_df = pd.read_csv(\"gold_standard_entities.csv\")\n",
    "\n",
    "def preprocess_entities(df, gold_col='entities_gold', machine_col='entities_ner'):\n",
    "    # Ensure that 'entities_gold' and 'entities_ner' columns are dictionaries\n",
    "    df[gold_col] = df[gold_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df[machine_col] = df[machine_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Extract and create GPE, LOC, FAC, ORG columns from 'entities_gold'\n",
    "    for etype in ['GPE', 'LOC', 'FAC', 'ORG']:\n",
    "        df[etype] = df[gold_col].apply(lambda x: x.get(etype, []))\n",
    "\n",
    "    # Extract and create machine_GPE, machine_LOC, machine_FAC, machine_ORG columns from 'entities_ner'\n",
    "    for etype in ['GPE', 'LOC', 'FAC', 'ORG']:\n",
    "        df[f'machine_{etype}'] = df[machine_col].apply(lambda x: x.get(etype, []))\n",
    "\n",
    "    # Convert empty lists to NaN for both sets of columns\n",
    "    for etype in ['GPE', 'LOC', 'FAC', 'ORG']:\n",
    "        df[etype] = df[etype].apply(lambda x: np.nan if not x else x)\n",
    "        df[f'machine_{etype}'] = df[f'machine_{etype}'].apply(lambda x: np.nan if not x else x)\n",
    "\n",
    "    # Set 'ORG' to NaN where any of 'FAC', 'LOC', or 'GPE' is not NaN (for both gold and machine entities)\n",
    "    df['ORG'] = df.apply(lambda row: np.nan if row[['FAC', 'LOC', 'GPE']].notna().any() else row['ORG'], axis=1)\n",
    "    df['machine_ORG'] = df.apply(lambda row: np.nan if row[[f'machine_{etype}' for etype in ['FAC', 'LOC', 'GPE']]].notna().any() else row['machine_ORG'], axis=1)\n",
    "\n",
    "    # Create arrays (lists) for the gold and machine entities\n",
    "    df['gold_entities_array'] = df.apply(lambda row: [row['GPE'], row['LOC'], row['FAC'], row['ORG']], axis=1)\n",
    "    df['machine_entities_array'] = df.apply(lambda row: [row['machine_GPE'], row['machine_LOC'], row['machine_FAC'], row['machine_ORG']], axis=1)\n",
    "\n",
    "    # Flatten the arrays and remove NaNs\n",
    "    df['gold_entities_array'] = df['gold_entities_array'].apply(lambda x: [item for sublist in x if isinstance(sublist, list) for item in sublist])\n",
    "    df['machine_entities_array'] = df['machine_entities_array'].apply(lambda x: [item for sublist in x if isinstance(sublist, list) for item in sublist])\n",
    "\n",
    "    # Select only the relevant columns\n",
    "    df = df[['doc', 'gold_entities_array', 'machine_entities_array']]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process the DataFrame\n",
    "processed_df = preprocess_entities(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566\n",
      "543\n"
     ]
    }
   ],
   "source": [
    "# now it's on the whole 100 article sample\n",
    "processed_df_long = processed_df.explode('machine_entities_array')\n",
    "processed_df_long = processed_df_long.drop_duplicates(subset=['doc', 'machine_entities_array'], keep='first')\n",
    "print(len(processed_df_long))\n",
    "\n",
    "# List of entities to remove (while above we did that manually, since then I created this csv file with a longer list of entities to remove)\n",
    "entities_to_remove = pd.read_csv(\"temp/entities_to_remove.csv\")\n",
    "entities_to_remove = entities_to_remove['value'].tolist()\n",
    "\n",
    "# Filter the DataFrame\n",
    "processed_df_long = processed_df_long[~processed_df_long['machine_entities_array'].isin(entities_to_remove)]\n",
    "print(len(processed_df_long))\n",
    "\n",
    "processed_df_long['toponym_lowercased'] = processed_df_long['machine_entities_array'].str.lower()\n",
    "# Convert list-like columns to strings (if applicable)\n",
    "processed_df_long['machine_entities_array'] = processed_df_long['machine_entities_array'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "processed_df_long['gold_entities_array'] = processed_df_long['gold_entities_array'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if liverpool is in the list to_remove\n",
    "print('birmingham' in entities_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/t7k_ys017hngt6d2bffnmzlw0000gn/T/ipykernel_9663/1559464168.py:2: DtypeWarning: Columns (0,1,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  toponym_candidates = pd.read_csv(\"temp/toponym_candidates_with_lads.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Merge with candidates\n",
    "toponym_candidates = pd.read_csv(\"toponym_candidates_with_lads.csv\")\n",
    "candidates_pool = processed_df_long.merge(toponym_candidates, left_on='toponym_lowercased', right_on='values', how='left')\n",
    "# group by machine_entity_array and doc and list the unique LAD24NM (do not drop other columns)\n",
    "grouped_df = candidates_pool.groupby(['machine_entities_array', 'doc', 'toponym_lowercased', 'clean_name', 'gold_entities_array']).agg({\n",
    "    'LAD24NM': list,       # Create list of LAD24NM\n",
    "    'POSTCODE_DISTRICT': list,      # Create list of geometry\n",
    "}).reset_index()\n",
    "\n",
    "# for each doc, assign a unique ID\n",
    "grouped_df['doc_id'] = grouped_df.groupby('doc').ngroup()\n",
    "# grouped_df.to_csv(\"candidates_classification_test_on_100_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for LabelStudio / Prodigy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>machine_entities_array</th>\n",
       "      <th>LAD24NM</th>\n",
       "      <th>POSTCODE_DISTRICT</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>SW1A</td>\n",
       "      <td>51.504607</td>\n",
       "      <td>-0.132177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>North Yorkshire</td>\n",
       "      <td>BD24</td>\n",
       "      <td>54.079897</td>\n",
       "      <td>-2.284160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Thanet</td>\n",
       "      <td>CT9</td>\n",
       "      <td>51.382937</td>\n",
       "      <td>1.389221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Dorset</td>\n",
       "      <td>DT2</td>\n",
       "      <td>50.749692</td>\n",
       "      <td>-2.450836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Boston</td>\n",
       "      <td>PE20</td>\n",
       "      <td>52.931083</td>\n",
       "      <td>-0.103562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6097</th>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>Gwynedd</td>\n",
       "      <td>LL48</td>\n",
       "      <td>52.936185</td>\n",
       "      <td>-4.069524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>West Northamptonshire</td>\n",
       "      <td>NN12</td>\n",
       "      <td>52.126196</td>\n",
       "      <td>-0.999142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6099</th>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>Cherwell</td>\n",
       "      <td>OX16</td>\n",
       "      <td>52.062293</td>\n",
       "      <td>-1.342842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>West Oxfordshire</td>\n",
       "      <td>OX18</td>\n",
       "      <td>51.762965</td>\n",
       "      <td>-1.591496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6101</th>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>Dorset</td>\n",
       "      <td>SP7</td>\n",
       "      <td>51.005020</td>\n",
       "      <td>-2.184370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3867 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    doc  \\\n",
       "0     Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "1     Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "2     Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "3     Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "4     Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "...                                                 ...   \n",
       "6097  A group of around 20 women and men protested a...   \n",
       "6098  A group of around 20 women and men protested a...   \n",
       "6099  A group of around 20 women and men protested a...   \n",
       "6100  A group of around 20 women and men protested a...   \n",
       "6101  A group of around 20 women and men protested a...   \n",
       "\n",
       "     machine_entities_array                LAD24NM POSTCODE_DISTRICT  \\\n",
       "0         10 Downing Street            Westminster              SW1A   \n",
       "1                    Labour        North Yorkshire              BD24   \n",
       "2                    Labour                 Thanet               CT9   \n",
       "3                    Labour                 Dorset               DT2   \n",
       "4                    Labour                 Boston              PE20   \n",
       "...                     ...                    ...               ...   \n",
       "6097          the Town Hall                Gwynedd              LL48   \n",
       "6098          the Town Hall  West Northamptonshire              NN12   \n",
       "6099          the Town Hall               Cherwell              OX16   \n",
       "6100          the Town Hall       West Oxfordshire              OX18   \n",
       "6101          the Town Hall                 Dorset               SP7   \n",
       "\n",
       "       Latitude  Longitude  \n",
       "0     51.504607  -0.132177  \n",
       "1     54.079897  -2.284160  \n",
       "2     51.382937   1.389221  \n",
       "3     50.749692  -2.450836  \n",
       "4     52.931083  -0.103562  \n",
       "...         ...        ...  \n",
       "6097  52.936185  -4.069524  \n",
       "6098  52.126196  -0.999142  \n",
       "6099  52.062293  -1.342842  \n",
       "6100  51.762965  -1.591496  \n",
       "6101  51.005020  -2.184370  \n",
       "\n",
       "[3867 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_pool_simple = candidates_pool[['doc', 'machine_entities_array', 'LAD24NM', 'POSTCODE_DISTRICT', 'Latitude', 'Longitude']]\n",
    "candidates_pool_simple\n",
    "# group by doc and machine_entities_array and remove duplicates\n",
    "grouped_candidates_pool = candidates_pool_simple.drop_duplicates(subset=['doc', 'machine_entities_array', 'LAD24NM'], keep='first')\n",
    "grouped_candidates_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_candidates_pool.to_csv(\"candidates_pool.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# in R, in Toponym_disambiguation.R I added metadata to this file, and will now load it back\n",
    "grouped_candidates_pool = pd.read_csv(\"annotators_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>...1</th>\n",
       "      <th>doc</th>\n",
       "      <th>machine_entities_array</th>\n",
       "      <th>LAD24NM</th>\n",
       "      <th>POSTCODE_DISTRICT</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>group_articles</th>\n",
       "      <th>domain</th>\n",
       "      <th>date</th>\n",
       "      <th>Main_LAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>SW1A</td>\n",
       "      <td>51.504607</td>\n",
       "      <td>-0.132177</td>\n",
       "      <td>article-1506227</td>\n",
       "      <td>scotsman.com</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>City of Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>North Yorkshire</td>\n",
       "      <td>BD24</td>\n",
       "      <td>54.079897</td>\n",
       "      <td>-2.284160</td>\n",
       "      <td>article-1506227</td>\n",
       "      <td>scotsman.com</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>City of Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Thanet</td>\n",
       "      <td>CT9</td>\n",
       "      <td>51.382937</td>\n",
       "      <td>1.389221</td>\n",
       "      <td>article-1506227</td>\n",
       "      <td>scotsman.com</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>City of Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Dorset</td>\n",
       "      <td>DT2</td>\n",
       "      <td>50.749692</td>\n",
       "      <td>-2.450836</td>\n",
       "      <td>article-1506227</td>\n",
       "      <td>scotsman.com</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>City of Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Rishi Sunak departs 10 Downing Street ahead of...</td>\n",
       "      <td>Labour</td>\n",
       "      <td>Boston</td>\n",
       "      <td>PE20</td>\n",
       "      <td>52.931083</td>\n",
       "      <td>-0.103562</td>\n",
       "      <td>article-1506227</td>\n",
       "      <td>scotsman.com</td>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>City of Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>6097</td>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>Gwynedd</td>\n",
       "      <td>LL48</td>\n",
       "      <td>52.936185</td>\n",
       "      <td>-4.069524</td>\n",
       "      <td>article-151086</td>\n",
       "      <td>redditchadvertiser.co.uk</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Redditch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>6098</td>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>West Northamptonshire</td>\n",
       "      <td>NN12</td>\n",
       "      <td>52.126196</td>\n",
       "      <td>-0.999142</td>\n",
       "      <td>article-151086</td>\n",
       "      <td>redditchadvertiser.co.uk</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Redditch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>6099</td>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>Cherwell</td>\n",
       "      <td>OX16</td>\n",
       "      <td>52.062293</td>\n",
       "      <td>-1.342842</td>\n",
       "      <td>article-151086</td>\n",
       "      <td>redditchadvertiser.co.uk</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Redditch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3865</th>\n",
       "      <td>6100</td>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>West Oxfordshire</td>\n",
       "      <td>OX18</td>\n",
       "      <td>51.762965</td>\n",
       "      <td>-1.591496</td>\n",
       "      <td>article-151086</td>\n",
       "      <td>redditchadvertiser.co.uk</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Redditch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3866</th>\n",
       "      <td>6101</td>\n",
       "      <td>A group of around 20 women and men protested a...</td>\n",
       "      <td>the Town Hall</td>\n",
       "      <td>Dorset</td>\n",
       "      <td>SP7</td>\n",
       "      <td>51.005020</td>\n",
       "      <td>-2.184370</td>\n",
       "      <td>article-151086</td>\n",
       "      <td>redditchadvertiser.co.uk</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>Redditch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3867 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ...1                                                doc  \\\n",
       "0        0  Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "1        1  Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "2        2  Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "3        3  Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "4        4  Rishi Sunak departs 10 Downing Street ahead of...   \n",
       "...    ...                                                ...   \n",
       "3862  6097  A group of around 20 women and men protested a...   \n",
       "3863  6098  A group of around 20 women and men protested a...   \n",
       "3864  6099  A group of around 20 women and men protested a...   \n",
       "3865  6100  A group of around 20 women and men protested a...   \n",
       "3866  6101  A group of around 20 women and men protested a...   \n",
       "\n",
       "     machine_entities_array                LAD24NM POSTCODE_DISTRICT  \\\n",
       "0         10 Downing Street            Westminster              SW1A   \n",
       "1                    Labour        North Yorkshire              BD24   \n",
       "2                    Labour                 Thanet               CT9   \n",
       "3                    Labour                 Dorset               DT2   \n",
       "4                    Labour                 Boston              PE20   \n",
       "...                     ...                    ...               ...   \n",
       "3862          the Town Hall                Gwynedd              LL48   \n",
       "3863          the Town Hall  West Northamptonshire              NN12   \n",
       "3864          the Town Hall               Cherwell              OX16   \n",
       "3865          the Town Hall       West Oxfordshire              OX18   \n",
       "3866          the Town Hall                 Dorset               SP7   \n",
       "\n",
       "       Latitude  Longitude   group_articles                    domain  \\\n",
       "0     51.504607  -0.132177  article-1506227              scotsman.com   \n",
       "1     54.079897  -2.284160  article-1506227              scotsman.com   \n",
       "2     51.382937   1.389221  article-1506227              scotsman.com   \n",
       "3     50.749692  -2.450836  article-1506227              scotsman.com   \n",
       "4     52.931083  -0.103562  article-1506227              scotsman.com   \n",
       "...         ...        ...              ...                       ...   \n",
       "3862  52.936185  -4.069524   article-151086  redditchadvertiser.co.uk   \n",
       "3863  52.126196  -0.999142   article-151086  redditchadvertiser.co.uk   \n",
       "3864  52.062293  -1.342842   article-151086  redditchadvertiser.co.uk   \n",
       "3865  51.762965  -1.591496   article-151086  redditchadvertiser.co.uk   \n",
       "3866  51.005020  -2.184370   article-151086  redditchadvertiser.co.uk   \n",
       "\n",
       "            date           Main_LAD  \n",
       "0     2022-11-15  City of Edinburgh  \n",
       "1     2022-11-15  City of Edinburgh  \n",
       "2     2022-11-15  City of Edinburgh  \n",
       "3     2022-11-15  City of Edinburgh  \n",
       "4     2022-11-15  City of Edinburgh  \n",
       "...          ...                ...  \n",
       "3862  2021-10-08           Redditch  \n",
       "3863  2021-10-08           Redditch  \n",
       "3864  2021-10-08           Redditch  \n",
       "3865  2021-10-08           Redditch  \n",
       "3866  2021-10-08           Redditch  \n",
       "\n",
       "[3867 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_candidates_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def create_labelstudio_json(df):\n",
    "    tasks = []\n",
    "\n",
    "    # Group by 'doc' and 'machine_entities_array'\n",
    "    grouped = df.groupby(['doc', 'machine_entities_array'])\n",
    "\n",
    "    for (doc, machine_entity), group in grouped:\n",
    "        # Collect all LAD24NM options for this doc-entity pair\n",
    "        options = [\n",
    "            {\"value\": row['LAD24NM']} for _, row in group.iterrows() if pd.notna(row['LAD24NM'])\n",
    "        ]\n",
    "        \n",
    "        # Add the additional fixed options, formatted correctly\n",
    "        additional_options = [\n",
    "            {\"value\": \"LAD not in options\"},\n",
    "            {\"value\": \"Entity is not a location\"},\n",
    "            {\"value\": \"Entity is a location outside the UK\"},\n",
    "            {\"value\": \"Entity spans across several districts (e.g., a region)\"},\n",
    "            {\"value\": \"Unsure\"}\n",
    "        ]\n",
    "        \n",
    "        # Combine the options\n",
    "        options += additional_options\n",
    "        \n",
    "        # Create the task in Label Studio format\n",
    "        task = {\n",
    "            \"data\": {\n",
    "                \"id\": len(tasks),  # Assign a unique id based on the number of tasks\n",
    "                \"doc\": doc,\n",
    "                \"machine_entities_array\": machine_entity,\n",
    "                \"main_LAD\": group['Main_LAD'].iloc[0],  # Take the first instance for metadata\n",
    "                \"domain\": group['domain'].iloc[0],\n",
    "                \"options\": options  # This is a list of dictionaries\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Convert list of tasks to JSON\n",
    "    return json.dumps(tasks, indent=4)\n",
    "\n",
    "# Example usage\n",
    "grouped_candidates_pool.fillna('Not linked to anything', inplace=True)  # Replace NaN values\n",
    "labelstudio_json = create_labelstudio_json(grouped_candidates_pool)\n",
    "\n",
    "# Save the JSON to a file for Label Studio\n",
    "with open('labelstudio_tasks.json', 'w') as f:\n",
    "    f.write(labelstudio_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhancing with entity position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def extract_filtered_entities_with_positions(doc):\n",
    "    \"\"\"\n",
    "    Extract entity positions from the filtered machine entities array.\n",
    "    \n",
    "    Args:\n",
    "    doc (spacy Doc): The original spacy document.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of entity data, including text, start, end, label, and other metadata.\n",
    "    \"\"\"\n",
    "    filtered_entities = []\n",
    "    entities = {\"GPE\": [], \"LOC\": [], \"FAC\": [], \"ORG\": []}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in entities:  # Check if the entity type is one we want to collect\n",
    "            entity_data = {\n",
    "                \"id\": ent.text,  # unique identifier can be ent.text or generate one dynamically\n",
    "                \"doc\": doc.text,\n",
    "                \"from_name\": \"label\",\n",
    "                \"to_name\": \"text\",\n",
    "                \"type\": \"labels\",\n",
    "                \"value\": {\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char,\n",
    "                    \"text\": ent.text,\n",
    "                    \"labels\": [ent.label_]  # GPE, LOC, FAC, ORG, etc.\n",
    "                }\n",
    "            }\n",
    "            filtered_entities.append(entity_data)\n",
    "\n",
    "    # Filter out ORG if FAC, LOC, or GPE is present\n",
    "    has_fac_loc_gpe = any(ent[\"value\"][\"labels\"][0] in [\"FAC\", \"LOC\", \"GPE\"] for ent in filtered_entities)\n",
    "    if has_fac_loc_gpe:\n",
    "        filtered_entities = [ent for ent in filtered_entities if ent[\"value\"][\"labels\"][0] != \"ORG\"]\n",
    "\n",
    "    return filtered_entities\n",
    "\n",
    "\n",
    "def process_docs_for_labelstudio(docs_ner, model_version=\"en_core_web_trf\"):\n",
    "    \"\"\"\n",
    "    Convert spacy docs to LabelStudio JSON format.\n",
    "    \n",
    "    Args:\n",
    "    docs_ner (list): List of spacy Docs.\n",
    "    model_version (str): The version of the model used.\n",
    "    score (float): The model confidence score (if available, otherwise default 0.0).\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tasks ready to be exported to LabelStudio format.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    \n",
    "    for doc in docs_ner:\n",
    "        # Extract only entities that passed filtering\n",
    "        entity_data = extract_filtered_entities_with_positions(doc)\n",
    "        \n",
    "        task = {\n",
    "            \"data\": {\n",
    "                \"doc\": doc.text\n",
    "            },\n",
    "            \"predictions\": [\n",
    "                {\n",
    "                    \"model_version\": model_version,\n",
    "                    \"result\": entity_data\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "# Example usage with spacy Docs:\n",
    "# Load your spacy docs\n",
    "docs_ner = DocBin().from_disk(\"bert.spacy\")\n",
    "docs_ner = list(docs_ner.get_docs(nlp.vocab))\n",
    "\n",
    "labelstudio_tasks = process_docs_for_labelstudio(docs_ner)\n",
    "\n",
    "# Save the JSON to a file for Label Studio\n",
    "with open('labelstudio_predictions.json', 'w') as f:\n",
    "    json.dump(labelstudio_tasks, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the two JSON files\n",
    "with open('labelstudio_predictions.json', 'r') as f1:  # length 96\n",
    "    predictions_data = json.load(f1)\n",
    "\n",
    "with open('labelstudio_tasks.json', 'r') as f2:  # length 543\n",
    "    options_metadata_data = json.load(f2)\n",
    "\n",
    "# Create a lookup dictionary from the predictions based on 'doc'\n",
    "predictions_lookup = {entry['data']['doc']: entry for entry in predictions_data}\n",
    "\n",
    "# Merge options/metadata with predictions based on the common 'doc'\n",
    "merged_data = []\n",
    "\n",
    "for task_entry in options_metadata_data:\n",
    "    doc_text = task_entry['data']['doc']\n",
    "    \n",
    "    # Start with the task entry\n",
    "    merged_entry = task_entry.copy()\n",
    "\n",
    "    # Check if the doc exists in the predictions JSON\n",
    "    if doc_text in predictions_lookup:\n",
    "        # Merge the corresponding prediction entry with the task entry\n",
    "        merged_entry['predictions'] = predictions_lookup[doc_text]['predictions']  # Copy the predictions\n",
    "    else:\n",
    "        merged_entry['predictions'] = []  # If no prediction found, initialize as empty list\n",
    "\n",
    "    merged_data.append(merged_entry)\n",
    "\n",
    "# Remove 'doc' from 'predictions' results in merged_data\n",
    "for prediction in merged_data:\n",
    "    if 'predictions' in prediction:  # Ensure 'predictions' key exists\n",
    "        for result in prediction['predictions']:\n",
    "            if 'result' in result:  # Ensure 'result' key exists\n",
    "                for res in result['result']:\n",
    "                    if \"doc\" in res:\n",
    "                        del res[\"doc\"]\n",
    "\n",
    "# Replace with _ any blank space from the entry id\n",
    "for entry in merged_data:\n",
    "    for prediction in entry['predictions']:\n",
    "        for result in prediction['result']:\n",
    "            result['id'] = result['id'].replace(' ', '_')\n",
    "\n",
    "# Save the merged data to a new JSON file\n",
    "with open('merged_data.json', 'w') as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "\n",
    "print(\"Merge completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the merged data\n",
    "with open('merged_data.json', 'r') as infile:\n",
    "    merged_data = json.load(infile)\n",
    "\n",
    "# Function to update labels based on machine_entities_array\n",
    "def update_labels(merged_data):\n",
    "    for item in merged_data:\n",
    "        machine_entities_array = item['data']['machine_entities_array']\n",
    "        for prediction in item['predictions']:\n",
    "            for result in prediction['result']:\n",
    "                if result['value']['text'] == machine_entities_array:\n",
    "                    result['value']['labels'] = [\"Entity\"]\n",
    "                else:\n",
    "                    result['value']['labels'] = [\"Other\"]\n",
    "    return merged_data\n",
    "\n",
    "# Update the labels in merged_data\n",
    "updated_data = update_labels(merged_data)\n",
    "\n",
    "# Save the updated data to a new JSON file\n",
    "with open('updated_merged_data.json', 'w') as outfile:\n",
    "    json.dump(updated_data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two annotators have annotated the above JSON file, 543 tasks, in Label Studio. Now we import the results and calculate Cohen's Kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annotated_disamb.json', 'r') as infile:\n",
    "    annotation_results = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_results\n",
    "\n",
    "def extract_annotations(annotation_results):\n",
    "    annotations = {}\n",
    "    for item in annotation_results:\n",
    "        annotator = item['annotator']\n",
    "        if annotator not in annotations:\n",
    "            annotations[annotator] = []\n",
    "        annotations[annotator].append(item['options'])\n",
    "    return annotations\n",
    "\n",
    "# Extract the annotations\n",
    "extracted_annotations = extract_annotations(annotation_results)\n",
    "\n",
    "# Split in two arrays\n",
    "y1 = extracted_annotations['oscarbovin@hotmail.se']\n",
    "y2 = extracted_annotations['s.bisiani@surrey.ac.uk']\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Convert the lists to strings\n",
    "y1 = [str(item) for item in y1]\n",
    "y2 = [str(item) for item in y2]\n",
    "\n",
    "cohen_kappa_score(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for elements in y1 and y2 that are not the same, comparing them by position\n",
    "diffs = [(i, y1[i], y2[i]) for i in range(len(y1)) if y1[i] != y2[i]]\n",
    "diffs_csv = pd.DataFrame(diffs, columns=['index', 'y1', 'y2'])\n",
    "diffs_csv.head(27)\n",
    "\n",
    "# Create a dictionary with the correct values\n",
    "correct_values = {\n",
    "    42: 'LAD not in options',\n",
    "    43: 'Entity spans across several districts (e.g., a region)',\n",
    "    57: 'South Lanarkshire',\n",
    "    67: 'Gloucester',\n",
    "    69: 'LAD not in options',\n",
    "    77: 'Entity spans across several districts (e.g., a region)',\n",
    "    95: 'Wandsworth',\n",
    "    102: 'Entity spans across several districts (e.g., a region)',\n",
    "    113: 'Entity is a location outside the UK',\n",
    "    159: 'Entity spans across several districts (e.g., a region)',\n",
    "    183: 'Entity is a location outside the UK',\n",
    "    193: 'Entity is not a location',\n",
    "    218: 'Entity spans across several districts (e.g., a region)',\n",
    "    222: 'Entity spans across several districts (e.g., a region)',\n",
    "    230: 'Entity spans across several districts (e.g., a region)',\n",
    "    232: 'Armagh City, Banbridge and Craigavon',\n",
    "    279: 'LAD not in options',\n",
    "    309: \"King's Lynn and West Norfolk\",\n",
    "    313: 'Rugby',\n",
    "    413: 'Entity is not a location',\n",
    "    414: 'Newark and Sherwood',\n",
    "    454: 'Entity is not a location',\n",
    "    455: 'Entity is not a location',\n",
    "    457: 'Entity spans across several districts (e.g., a region)',\n",
    "    505: 'Entity is not a location',\n",
    "    507: 'North Yorkshire',\n",
    "    528: 'LAD not in options'\n",
    "}\n",
    "\n",
    "# make a new array with the correct values\n",
    "corrected_y2 = y2.copy()\n",
    "for index, value in correct_values.items():\n",
    "    corrected_y2[index] = value\n",
    "\n",
    "# create df from original annotated data json\n",
    "annotated_data = pd.DataFrame(annotation_results)\n",
    "annotated_data = annotated_data.groupby('id').sample(n=1, random_state=1).reset_index(drop=True)\n",
    "annotated_data['annotators_choice'] = corrected_y2\n",
    "annotated_data = annotated_data.drop(columns=['options', 'annotator', 'annotation_id', 'created_at', 'updated_at', 'lead_time', 'agreement'])\n",
    "annotated_data.to_csv(\"annotators_disambiguated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract entity details\n",
    "def extract_entities(row):\n",
    "    entities = []\n",
    "    for entity_type, entity_list in row['entities'].items():\n",
    "        for entity in entity_list:\n",
    "            start_pos = row['doc'].find(entity)\n",
    "            if start_pos != -1:\n",
    "                end_pos = start_pos + len(entity)\n",
    "                entities.append({\n",
    "                    \"start\": start_pos,\n",
    "                    \"end\": end_pos,\n",
    "                    \"entity_type\": entity_type\n",
    "                })\n",
    "    return entities\n",
    "\n",
    "# Create a dictionary to store the JSON structure\n",
    "json_data = {}\n",
    "\n",
    "# Iterate through the DataFrame and populate the dictionary\n",
    "for index, row in ner_entities_sample.iterrows():\n",
    "    json_data[row['doc']] = extract_entities(row)\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open('entities.json', 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
